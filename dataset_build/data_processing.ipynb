{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "from urllib.parse import urlparse\n",
    "from collections import Counter\n",
    "from scrapper import Scrapper\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import multiprocessing as multi\n",
    "from multiprocessing import Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "with open(r'C:\\Users\\Anshul\\Downloads\\newsroom-thin.tar\\newsroom-thin\\train.jsonl\\train-shell.jsonl', 'r') as f:\n",
    "    for jline, i in zip(f.readlines(), range(100_000)):\n",
    "        obj = json.loads(jline)\n",
    "        d = {\n",
    "            'id': i,\n",
    "            'link': obj['archive']\n",
    "        }\n",
    "        links.append(d)\n",
    "\n",
    "with open(r'links.csv', 'w', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=['id','link'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = []\n",
    "id_domain_list = []\n",
    "with open(r'links.csv', 'r') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for each in reader:\n",
    "        httpsI = each['link'].rfind('https')\n",
    "        if httpsI == -1 or httpsI == 0:\n",
    "            httpsI = each['link'].rfind('http')\n",
    "        link = each['link'][httpsI:]\n",
    "        domain = urlparse(link).netloc\n",
    "        id_domain = {\n",
    "            'id': each['id'],\n",
    "            'domain': domain\n",
    "        }\n",
    "        id_domain_list.append(id_domain)\n",
    "        domains.append(domain)\n",
    "domains = Counter(domains)\n",
    "s = 0\n",
    "selected_domains = []\n",
    "for w in sorted(domains, key=domains.get, reverse=True):\n",
    "    s += domains[w]\n",
    "    selected_domains.append(w)\n",
    "    if s >= 60_000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['www.nytimes.com',\n",
       " 'www.washingtonpost.com',\n",
       " 'www.foxnews.com',\n",
       " 'www.theguardian.com',\n",
       " 'www.nydailynews.com',\n",
       " 'www.cnn.com',\n",
       " 'mashable.com',\n",
       " 'www.forbes.com',\n",
       " 'www.wsj.com',\n",
       " 'time.com',\n",
       " 'www.cnbc.com',\n",
       " 'content.usatoday.com',\n",
       " 'www.usatoday.com',\n",
       " 'www.9news.com.au',\n",
       " 'www.tmz.com']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_ids = []\n",
    "with open(r'links.csv', 'r') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for each in reader:\n",
    "        httpsI = each['link'].rfind('https')\n",
    "        if httpsI == -1 or httpsI == 0:\n",
    "            httpsI = each['link'].rfind('http')\n",
    "        link = each['link'][httpsI:]\n",
    "        domain = urlparse(link).netloc\n",
    "        if domain in selected_domains:\n",
    "            dict_ = {\n",
    "                'id': each['id'],\n",
    "                'link': each['link']\n",
    "            }\n",
    "            selected_ids.append(dict_)\n",
    "df = pd.DataFrame().from_dict(selected_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://web.archive.org/web/2016040819id_/http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>https://web.archive.org/web/2014072519id_/http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>https://web.archive.org/web/2007052219id_/http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>https://web.archive.org/web/2016041519id_/http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>https://web.archive.org/web/2015090619id_/http...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                               link\n",
       "0  0  https://web.archive.org/web/2016040819id_/http...\n",
       "1  1  https://web.archive.org/web/2014072519id_/http...\n",
       "2  5  https://web.archive.org/web/2007052219id_/http...\n",
       "3  6  https://web.archive.org/web/2016041519id_/http...\n",
       "4  8  https://web.archive.org/web/2015090619id_/http..."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('selected_links.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://web.archive.org/web/2016040819id_/http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>https://web.archive.org/web/2014072519id_/http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>https://web.archive.org/web/2007052219id_/http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>https://web.archive.org/web/2016041519id_/http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>https://web.archive.org/web/2015090619id_/http...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               link\n",
       "0   0  https://web.archive.org/web/2016040819id_/http...\n",
       "1   1  https://web.archive.org/web/2014072519id_/http...\n",
       "2   5  https://web.archive.org/web/2007052219id_/http...\n",
       "3   6  https://web.archive.org/web/2016041519id_/http...\n",
       "4   8  https://web.archive.org/web/2015090619id_/http..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 15340, 30680, 46020, 61360]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(0,len(df)+1,(len(df))//4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(n, page_list):\n",
    "    \"\"\"Splits the list into n chunks\"\"\"\n",
    "    return np.array_split(page_list,n)\n",
    " \n",
    "\n",
    "def trgt_fun(df, queue):\n",
    "    sc = Scrapper()\n",
    "    x = []\n",
    "    for index, row in df.iterrows():\n",
    "        sc.setUrl(row['link'])\n",
    "        temp = {\n",
    "            'id': row['id'],\n",
    "            'text': sc.getArticle()\n",
    "        }\n",
    "        x.append(temp)\n",
    "    res = queue.get()\n",
    "    res.extend(x)\n",
    "    queue.put(res)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    df = pd.read_csv('selected_links.csv')\n",
    "    res = []\n",
    "\n",
    "    name = 'anshul'\n",
    "    mem_dict = {\n",
    "        'anshul':{\n",
    "            'l': 0,\n",
    "            'r': 15340\n",
    "        },\n",
    "        'timi':{\n",
    "            'l': 15340,\n",
    "            'r': 30680\n",
    "        },\n",
    "        'piyush':{\n",
    "            'l': 30680,\n",
    "            'r': 46020\n",
    "        },\n",
    "        'anish':{\n",
    "            'l': 46020,\n",
    "            'r': 61361\n",
    "        },\n",
    "    }\n",
    "\n",
    "    df = df.iloc[mem_dict[name]['l']:mem_dict[name]['r']]\n",
    "\n",
    "    '''\n",
    "    anshul - 0:15340\n",
    "    timi - 15340:30680\n",
    "    piyush - 30680:46020\n",
    "    anish - 46020:61361\n",
    "    '''\n",
    "    cpus = multi.cpu_count()\n",
    "\n",
    "    workers = []\n",
    "    page_list = df\n",
    "    page_bins = chunks(cpus, page_list)\n",
    "    queue = Queue()\n",
    "    queue.put(res)\n",
    "    print('Using {}/{} CPU(s)'.format(cpus,cpus))\n",
    "    for cpu in range(cpus):\n",
    "        sys.stdout.write(\"CPU \" + str(cpu) + \"\\n\")\n",
    "        worker = multi.Process(name=str(cpu), \n",
    "                            target=trgt_fun, \n",
    "                            args=(page_bins[cpu],queue))\n",
    "        worker.start()\n",
    "        workers.append(worker)\n",
    "\n",
    "    for worker in workers:\n",
    "        worker.join()\n",
    "\n",
    "    df1 = pd.DataFrame().from_records(queue.get())\n",
    "    df1.to_csv('works_{}.csv'.format(name))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "919465a1be88223d2082ba65fce417e0b570aa4ca644ce80797a6929da012101"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
